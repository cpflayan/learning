import os
import requests
from datetime import datetime
from zipfile import ZipFile
import io
import pandas as pd



def generate_monthly_urls(symbol, ktime, start_date, end_date):
    current = start_date
    urls = []
    while current <= end_date:
        url = f"https://data.binance.vision/data/futures/um/monthly/klines/{symbol}/{ktime}/{symbol}-{ktime}-{current.strftime('%Y-%m')}.zip"
        urls.append((url, current.strftime('%Y-%m')))
        if current.month == 12:
            current = current.replace(year=current.year + 1, month=1, day=1)
        else:
            current = current.replace(month=current.month + 1, day=1)
    return urls

def download_and_extract_zip(url):
    print(f"下載 {url} ...")
    r = requests.get(url, stream=True)
    if r.status_code != 200:
        print(f"下載失敗，狀態碼 {r.status_code}")
        return None
    with ZipFile(io.BytesIO(r.content)) as z:
        name_list = z.namelist()
        if len(name_list) == 0:
            print("壓縮包內沒有檔案")
            return None
        with z.open(name_list[0]) as f:
            df = pd.read_csv(f)  # 自動判斷header
    return df

def main():
    symbol = "ETHUSDT"
    ktime = "1m"
    start_date = datetime(2023, 5, 1)
    end_date = datetime(2025, 6, 1)

    urls = generate_monthly_urls(symbol, ktime, start_date, end_date)
    all_dfs = []

    for url, ym in urls:
        df = download_and_extract_zip(url)
        if df is not None and not df.empty:
            print(f"{ym} 資料樣本:")
            print(df.head())
            all_dfs.append(df)
        else:
            print(f"{ym} 資料下載失敗或為空，跳過。")

    if not all_dfs:
        print("沒有任何資料被下載，程式結束。")
        return

    combined_df = pd.concat(all_dfs, ignore_index=True)

    # 檢查欄位名稱是否存在
    if 'open_time' in combined_df.columns:
        try:
            combined_df['open_time'] = pd.to_datetime(combined_df['open_time'], unit='ms')
        except Exception as e:
            print(f"轉換時間欄位錯誤: {e}")
    else:
        print("找不到 'open_time' 欄位，請確認資料格式。")

    combined_df = combined_df.sort_values('open_time').reset_index(drop=True)

    print(f"總共合併 {len(combined_df)} 筆資料。")

    output_file = f'{symbol}_1m_klines_merged.csv'
    combined_df.to_csv(output_file, index=False)
    print(f"合併後CSV檔案已儲存：{output_file}")

if __name__ == "__main__":
    main()

#############################################################
import numpy as np
import pandas as pd
import gymnasium as gym
from gymnasium import spaces
from sklearn.preprocessing import StandardScaler
import joblib
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.env_checker import check_env


# --- 1. 資料前處理: 加入 Fibonacci 水平線特徵 ---
def add_fibonacci_levels(df, window=50):
    high = df['high'].rolling(window).max()
    low = df['low'].rolling(window).min()
    diff = high - low
    df['fib_236'] = high - diff * 0.236
    df['fib_382'] = high - diff * 0.382
    df['fib_618'] = high - diff * 0.618
    return df

# 這邊你自己替換成實際讀檔程式
df = pd.read_csv('ETHUSDT_1m_klines_merged.csv')  # 請改成你的檔案路徑
df.columns = df.columns.str.lower().str.strip()
df = add_fibonacci_levels(df)
df = df.dropna().reset_index(drop=True)

# 選擇要使用的特徵欄位
features = df[['close', 'volume', 'high', 'low', 'fib_236', 'fib_382', 'fib_618']].copy()
data_array = features.values.astype(np.float32)
print("資料形狀:", data_array.shape)

# --- 2. 標準化 ---
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data_array)
joblib.dump(scaler, 'scaler_rl.pkl')
print("已完成正規化:", scaled_data.shape)


class SimpleTradingEnv(gym.Env):
    def __init__(self, data):
        super().__init__()
        self.data = data
        self.max_step = len(data) - 1

        # 觀察空間：所有特徵 + 持倉狀態
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(data.shape[1] + 1,), dtype=np.float32)
        
        # 動作空間：0=持有(不動作), 1=買多開倉, 2=賣空開倉
        self.action_space = spaces.Discrete(3)

        self.position = 0  # 0=無倉位, 1=多單, -1=空單
        self.cash = 10000.0
        self.entry_price = 0.0
        self.entry_step = 0

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # 隨機起點避免過擬合
        self.current_step = np.random.randint(50, self.max_step - 201)
        self.position = 0
        self.cash = 10000.0
        self.entry_price = 0.0
        self.entry_step = 0
        obs = self._get_obs()
        return obs, {}

    def _get_obs(self):
        obs = self.data[self.current_step]
        return np.append(obs, self.position).astype(np.float32)

    def step(self, action):
        terminated = False
        truncated = False
        info = {}

        price = self.data[self.current_step][0]  # close 價格（標準化後）
        slippage_pct = np.random.uniform(-0.0003, 0.0003)
        executed_price = price * (1 + slippage_pct)
        fee_rate = 0.0004
        fee = executed_price * fee_rate
        reward = 0.0

        # === 開倉邏輯 ===
        if self.position == 0:
            if action in [1, 2]:
                self.position = 1 if action == 1 else -1
                self.entry_price = executed_price
                self.entry_step = self.current_step
                self.cash -= fee
                reward = -fee  # 開倉付出手續費
            else:
                 reward = 0.0  # 持有不動作
        else:
             # === 有倉位 ===
            holding_time = self.current_step - self.entry_step
            price_diff = executed_price - self.entry_price
            unrealized_pnl = price_diff * self.position

            # === 強制平倉（超時） ===
            if holding_time >= 200:
                pnl = unrealized_pnl
                total_fee = (executed_price + self.entry_price) * fee_rate
                net_pnl = pnl - total_fee
                self.cash += net_pnl
                reward = np.clip(net_pnl * 5, -10, 10)  # 平倉獎勵限制在 [-10, 10]
                self.position = 0
                self.entry_price = 0
                truncated = True
            else:
                # === 中間狀態：決定是否平倉或持有 ===
                if action == 0:
                    # 持有：給少量浮動獎勵 + 懲罰長時間持倉
                    reward = np.clip(unrealized_pnl * 0.5, -1, 1) - 0.001 * (holding_time / 200)
                else:
                    # 平倉再開倉
                    pnl = unrealized_pnl
                    total_fee = (executed_price + self.entry_price) * fee_rate
                    net_pnl = pnl - total_fee
                    self.cash += net_pnl
                    reward = np.clip(net_pnl * 5, -10, 10)

                    # 重設倉位
                    self.position = 0
                    self.entry_price = 0

                    # 再開新倉
                    if action in [1, 2]:
                        self.position = 1 if action == 1 else -1
                        self.entry_price = executed_price
                        self.entry_step = self.current_step
                        self.cash -= fee
                        reward -= fee

        # === 評估淨值 ===
        net_worth = self.cash + (self.position * (price - self.entry_price) if self.position != 0 else 0)

        # === 爆倉懲罰 ===
        if net_worth < 8000:
            reward -= 10  # 溫和但有力的懲罰
            terminated = True

        self.current_step += 1
        if self.current_step >= self.max_step:
            truncated = True


        
        obs = self._get_obs()
        reward = float(np.clip(reward, -20, 20))  # 避免極端獎勵值

        info['net_worth'] = net_worth
        info['position'] = self.position
        info['step'] = self.current_step

        return obs, reward, terminated, truncated, info

# --- 4. 訓練用 Callback 打印與儲存紀錄 ---
import pandas as pd
class TrainingLogger(BaseCallback):
    def __init__(self, log_path='log.csv', verbose=1):
        super().__init__(verbose)
        self.log_path = log_path
        self.logs = []

    def _on_step(self) -> bool:
        metrics = self.model.logger.name_to_value
        if 'rollout/ep_rew_mean' in metrics:
            entry = {
                'iteration': len(self.logs),
                'ep_rew_mean': metrics.get('rollout/ep_rew_mean', 0),
                'ep_len_mean': metrics.get('rollout/ep_len_mean', 0),
                'explained_variance': metrics.get('train/explained_variance', 0),
                'entropy_loss': metrics.get('train/entropy_loss', 0),
                'value_loss': metrics.get('train/value_loss', 0),
                'policy_gradient_loss': metrics.get('train/policy_gradient_loss', 0)
            }
            self.logs.append(entry)
            if len(self.logs) % 5 == 0 and self.verbose > 0:
                print(f"[Iter {entry['iteration']:>3}] Reward: {entry['ep_rew_mean']:.2f} | "
                      f"Len: {entry['ep_len_mean']:.0f} | ExplVar: {entry['explained_variance']:.2f} | "
                      f"Entropy: {entry['entropy_loss']:.2f}")
        return True

    def _on_training_end(self) -> None:
        pd.DataFrame(self.logs).to_csv(self.log_path, index=False)
        print(f"\n✅ 訓練完成，log 存於：{self.log_path}")
# 建立環境與模型部分同你原本：

env = SimpleTradingEnv(data=scaled_data)
env = Monitor(env)
check_env(env, warn=True)

model = PPO("MlpPolicy",
            env,
            learning_rate=3e-4,
            n_steps=512,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.02,
            vf_coef=0.5,
            max_grad_norm=0.5,
            verbose=1)

model.learn(total_timesteps=100_000, callback=TrainingLogger(log_path='log.csv'))

model.save("ppo_trading_model")
