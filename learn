import numpy as np
import pandas as pd
import gymnasium as gym
from gymnasium import spaces
from sklearn.preprocessing import StandardScaler
import joblib
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.env_checker import check_env


# --- 1. 資料前處理: 加入 Fibonacci 水平線特徵 ---
def add_fibonacci_levels(df, window=50):
    high = df['high'].rolling(window).max()
    low = df['low'].rolling(window).min()
    diff = high - low
    df['fib_236'] = high - diff * 0.236
    df['fib_382'] = high - diff * 0.382
    df['fib_618'] = high - diff * 0.618
    return df

# 這邊你自己替換成實際讀檔程式
df = pd.read_csv('ETHUSDT_1m_klines_merged.csv')  # 請改成你的檔案路徑
df.columns = df.columns.str.lower().str.strip()
df = add_fibonacci_levels(df)
df = df.dropna().reset_index(drop=True)

# 選擇要使用的特徵欄位
features = df[['close', 'volume', 'high', 'low', 'fib_236', 'fib_382', 'fib_618']].copy()
data_array = features.values.astype(np.float32)
print("資料形狀:", data_array.shape)

# --- 2. 標準化 ---
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data_array)
joblib.dump(scaler, 'scaler_rl.pkl')
print("已完成正規化:", scaled_data.shape)


class SimpleTradingEnv(gym.Env):
    def __init__(self, data):
        super().__init__()
        self.data = data
        self.max_step = len(data) - 1

        # 觀察空間：所有特徵 + 持倉狀態
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(data.shape[1] + 1,), dtype=np.float32)
        
        # 動作空間：0=持有(不動作), 1=買多開倉, 2=賣空開倉
        self.action_space = spaces.Discrete(3)

        self.position = 0  # 0=無倉位, 1=多單, -1=空單
        self.cash = 10000.0
        self.entry_price = 0.0
        self.entry_step = 0

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # 隨機起點避免過擬合
        self.current_step = np.random.randint(50, self.max_step - 201)
        self.position = 0
        self.cash = 10000.0
        self.entry_price = 0.0
        self.entry_step = 0
        obs = self._get_obs()
        return obs, {}

    def _get_obs(self):
        obs = self.data[self.current_step]
        return np.append(obs, self.position).astype(np.float32)

    def step(self, action):
        terminated = False
        truncated = False
        reward = 0.0
        info = {}

        price = self.data[self.current_step][0]  # close 價格（標準化後）
        slippage_pct = np.random.uniform(-0.0003, 0.0003)
        executed_price = price * (1 + slippage_pct)
        fee_rate = 0.0004
        fee = executed_price * fee_rate

        # 動作邏輯
        if self.position == 0:
            # 無倉位時才可開倉
            if action == 1:  # 買多
                self.position = 1
                self.entry_price = executed_price
                self.entry_step = self.current_step
                self.cash -= fee
                reward = -fee  # 付手續費為負獎勵
            elif action == 2:  # 賣空
                self.position = -1
                self.entry_price = executed_price
                self.entry_step = self.current_step
                self.cash -= fee
                reward = -fee
            else:  # 持有不動作
                reward = 0.0
        else:
            # 有倉位時，計算浮動獲利
            unrealized_pnl = (executed_price - self.entry_price) * self.position
            holding_time = self.current_step - self.entry_step
            time_penalty = -0.001 * (holding_time / 200)

            # 強制平倉條件：超過最大持倉時間
            if holding_time > 200:
                pnl = unrealized_pnl
                total_fee = (executed_price + self.entry_price) * fee_rate
                net_pnl = pnl - total_fee
                self.cash += net_pnl
                reward = net_pnl * 10  # 放大平倉獎勵/懲罰
                self.position = 0
                self.entry_price = 0
                truncated = True
            else:
                # 持有中：給浮動獎勵，鼓勵正收益，懲罰時間過長
                reward = unrealized_pnl * 2 + time_penalty

            # 如果使用者下持有動作，視為不平倉，繼續持有
            if action == 0:
                pass
            else:
                # 用戶嘗試開新倉，先自動平倉，再開新倉
                pnl = unrealized_pnl
                total_fee = (executed_price + self.entry_price) * fee_rate
                net_pnl = pnl - total_fee
                self.cash += net_pnl
                reward += net_pnl * 10  # 放大平倉獎勵
                self.position = 0
                self.entry_price = 0

                # 新開倉
                if action == 1:
                    self.position = 1
                    self.entry_price = executed_price
                    self.entry_step = self.current_step
                    self.cash -= fee
                    reward -= fee
                elif action == 2:
                    self.position = -1
                    self.entry_price = executed_price
                    self.entry_step = self.current_step
                    self.cash -= fee
                    reward -= fee

        # 計算資產淨值（現金 + 持倉未實現損益）
        net_worth = self.cash + (self.position * (price - self.entry_price) if self.position != 0 else 0)

        # 爆倉判斷
        if net_worth < 10000 * 0.8:
            reward -= 50  # 爆倉重懲罰
            terminated = True

        self.current_step += 1
        if self.current_step >= self.max_step:
            truncated = True

        obs = self._get_obs()
        reward = float(reward)

        info['net_worth'] = net_worth
        info['position'] = self.position
        info['step'] = self.current_step

        return obs, reward, terminated, truncated, info

# --- 4. 訓練用 Callback 打印與儲存紀錄 ---
import pandas as pd
class TrainingLogger(BaseCallback):
    def __init__(self, log_path='log.csv', verbose=1):
        super().__init__(verbose)
        self.log_path = log_path
        self.logs = []

    def _on_step(self) -> bool:
        metrics = self.model.logger.name_to_value
        if 'rollout/ep_rew_mean' in metrics:
            entry = {
                'iteration': len(self.logs),
                'ep_rew_mean': metrics.get('rollout/ep_rew_mean', 0),
                'ep_len_mean': metrics.get('rollout/ep_len_mean', 0),
                'explained_variance': metrics.get('train/explained_variance', 0),
                'entropy_loss': metrics.get('train/entropy_loss', 0),
                'value_loss': metrics.get('train/value_loss', 0),
                'policy_gradient_loss': metrics.get('train/policy_gradient_loss', 0)
            }
            self.logs.append(entry)
            if len(self.logs) % 5 == 0 and self.verbose > 0:
                print(f"[Iter {entry['iteration']:>3}] Reward: {entry['ep_rew_mean']:.2f} | "
                      f"Len: {entry['ep_len_mean']:.0f} | ExplVar: {entry['explained_variance']:.2f} | "
                      f"Entropy: {entry['entropy_loss']:.2f}")
        return True

    def _on_training_end(self) -> None:
        pd.DataFrame(self.logs).to_csv(self.log_path, index=False)
        print(f"\n✅ 訓練完成，log 存於：{self.log_path}")
# 建立環境與模型部分同你原本：

env = SimpleTradingEnv(data=scaled_data)
env = Monitor(env)
check_env(env, warn=True)

model = PPO("MlpPolicy",
            env,
            learning_rate=3e-4,
            n_steps=512,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.02,
            vf_coef=0.5,
            max_grad_norm=0.5,
            verbose=1)

model.learn(total_timesteps=100_000, callback=TrainingLogger(log_path='log.csv'))

model.save("ppo_trading_model")

