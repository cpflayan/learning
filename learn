import numpy as np
import pandas as pd
import gymnasium as gym
from gymnasium import spaces
from sklearn.preprocessing import StandardScaler
import joblib
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.env_checker import check_env

# --- Fibonacci 水平線函數略（同你原本）---

# --- 資料前處理（略），假設 scaled_data 已經準備好 ---

class SimpleTradingEnv(gym.Env):
    def __init__(self, data):
        super().__init__()
        self.data = data
        self.max_step = len(data) - 1

        # 觀察空間：所有特徵 + 持倉狀態
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(data.shape[1] + 1,), dtype=np.float32)
        
        # 動作空間：0=持有(不動作), 1=買多開倉, 2=賣空開倉
        self.action_space = spaces.Discrete(3)

        self.position = 0  # 0=無倉位, 1=多單, -1=空單
        self.cash = 10000.0
        self.entry_price = 0.0
        self.entry_step = 0

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # 隨機起點避免過擬合
        self.current_step = np.random.randint(50, self.max_step - 201)
        self.position = 0
        self.cash = 10000.0
        self.entry_price = 0.0
        self.entry_step = 0
        obs = self._get_obs()
        return obs, {}

    def _get_obs(self):
        obs = self.data[self.current_step]
        return np.append(obs, self.position).astype(np.float32)

    def step(self, action):
        terminated = False
        truncated = False
        reward = 0.0
        info = {}

        price = self.data[self.current_step][0]  # close 價格（標準化後）
        slippage_pct = np.random.uniform(-0.0003, 0.0003)
        executed_price = price * (1 + slippage_pct)
        fee_rate = 0.0004
        fee = executed_price * fee_rate

        # 動作邏輯
        if self.position == 0:
            # 無倉位時才可開倉
            if action == 1:  # 買多
                self.position = 1
                self.entry_price = executed_price
                self.entry_step = self.current_step
                self.cash -= fee
                reward = -fee  # 付手續費為負獎勵
            elif action == 2:  # 賣空
                self.position = -1
                self.entry_price = executed_price
                self.entry_step = self.current_step
                self.cash -= fee
                reward = -fee
            else:  # 持有不動作
                reward = 0.0
        else:
            # 有倉位時，計算浮動獲利
            unrealized_pnl = (executed_price - self.entry_price) * self.position
            holding_time = self.current_step - self.entry_step
            time_penalty = -0.001 * (holding_time / 200)

            # 強制平倉條件：超過最大持倉時間
            if holding_time > 200:
                pnl = unrealized_pnl
                total_fee = (executed_price + self.entry_price) * fee_rate
                net_pnl = pnl - total_fee
                self.cash += net_pnl
                reward = net_pnl * 10  # 放大平倉獎勵/懲罰
                self.position = 0
                self.entry_price = 0
                truncated = True
            else:
                # 持有中：給浮動獎勵，鼓勵正收益，懲罰時間過長
                reward = unrealized_pnl * 2 + time_penalty

            # 如果使用者下持有動作，視為不平倉，繼續持有
            if action == 0:
                pass
            else:
                # 用戶嘗試開新倉，先自動平倉，再開新倉
                pnl = unrealized_pnl
                total_fee = (executed_price + self.entry_price) * fee_rate
                net_pnl = pnl - total_fee
                self.cash += net_pnl
                reward += net_pnl * 10  # 放大平倉獎勵
                self.position = 0
                self.entry_price = 0

                # 新開倉
                if action == 1:
                    self.position = 1
                    self.entry_price = executed_price
                    self.entry_step = self.current_step
                    self.cash -= fee
                    reward -= fee
                elif action == 2:
                    self.position = -1
                    self.entry_price = executed_price
                    self.entry_step = self.current_step
                    self.cash -= fee
                    reward -= fee

        # 計算資產淨值（現金 + 持倉未實現損益）
        net_worth = self.cash + (self.position * (price - self.entry_price) if self.position != 0 else 0)

        # 爆倉判斷
        if net_worth < 10000 * 0.8:
            reward -= 50  # 爆倉重懲罰
            terminated = True

        self.current_step += 1
        if self.current_step >= self.max_step:
            truncated = True

        obs = self._get_obs()
        reward = float(reward)

        info['net_worth'] = net_worth
        info['position'] = self.position
        info['step'] = self.current_step

        return obs, reward, terminated, truncated, info

# --- Callback 同前不變 ---

# 建立環境與模型部分同你原本：

env = SimpleTradingEnv(data=scaled_data)
env = Monitor(env)
check_env(env, warn=True)

model = PPO("MlpPolicy",
            env,
            learning_rate=3e-4,
            n_steps=512,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.02,
            vf_coef=0.5,
            max_grad_norm=0.5,
            verbose=1)

model.learn(total_timesteps=100_000, callback=TrainingLogger(log_path='log.csv'))

model.save("ppo_trading_model")
